import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, Input, Model
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
print("æ­£åœ¨åŠ è½½æ•°æ®...")
# è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
try:
    X = np.load('X_train.npy')
    y = np.load('y_train.npy')
except FileNotFoundError:
    print("æ‰¾ä¸åˆ° X_train.npy æˆ– y_train.npyï¼Œè¯·å…ˆè¿è¡Œæ–‡ä»¶2ç”Ÿæˆæ•°æ®ã€‚")
    exit()

print(f"æ•°æ®å½¢çŠ¶: {X.shape}") # (Samples, 260, 6)

# åŠ¨æ€è·å–ç±»åˆ«æ•°é‡
num_classes = len(np.unique(y))
print(f"æ£€æµ‹åˆ°ç±»åˆ«æ•°é‡: {num_classes}")

# åˆ‡åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ==========================================
# 2. æ„å»º MiniResNet æ¨¡å‹ (æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†)
# ==========================================

def resnet_block(input_tensor, filters, kernel_size=3, stride=1):
    """
    å®šä¹‰ä¸€ä¸ªæ®‹å·®å—:
    x -> Conv -> BN -> ReLU -> Conv -> BN -> Add -> ReLU
      |_______________________________________^
    """
    # ç¬¬ä¸€å±‚å·ç§¯
    x = layers.Conv1D(filters, kernel_size, padding='same', strides=stride)(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # ç¬¬äºŒå±‚å·ç§¯
    x = layers.Conv1D(filters, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)

    # æ®‹å·®è¿æ¥ (Shortcut)
    # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼ˆæ¯”å¦‚æ­¥é•¿!=1 æˆ–è€… æ»¤æ³¢å™¨æ•°é‡å˜äº†ï¼‰ï¼Œéœ€è¦å¯¹è¾“å…¥åš 1x1 å·ç§¯æ¥è°ƒæ•´å½¢çŠ¶
    if stride != 1 or input_tensor.shape[-1] != filters:
        shortcut = layers.Conv1D(filters, 1, strides=stride, padding='same')(input_tensor)
        shortcut = layers.BatchNormalization()(shortcut)
    else:
        shortcut = input_tensor

    # ç›¸åŠ 
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    return x

def build_mini_resnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    # === åˆå§‹å¤„ç†å±‚ ===
    # å…ˆæŠŠç‰¹å¾å‡ç»´ï¼Œä¿ç•™æ—¶é—´ä¿¡æ¯
    x = layers.Conv1D(16, 7, strides=2, padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    # x = layers.MaxPooling1D(3, strides=2, padding='same')(x) # å¯é€‰ï¼šå¦‚æœè¦è¿›ä¸€æ­¥å‹ç¼©

    # === æ®‹å·®å—å †å  (Miniç‰ˆ) ===
    # Block 1: ä¿æŒç‰¹å¾å›¾å¤§å°
    x = resnet_block(x, filters=16, stride=1)
    x = layers.Dropout(0.2)(x) # åŠ ä¸ŠDropouté˜²æ­¢è¿‡æ‹Ÿåˆ

    # Block 2: å¢åŠ é€šé“ï¼Œå‹ç¼©æ—¶é—´ç»´åº¦ (stride=2)
    x = resnet_block(x, filters=32, stride=2)
    x = layers.Dropout(0.2)(x)

    # Block 3: å†å¢åŠ é€šé“ï¼Œå†å‹ç¼©
    x = resnet_block(x, filters=64, stride=2)
    x = layers.Dropout(0.2)(x)

    # === è¾“å‡ºå±‚ ===
    x = layers.GlobalAveragePooling1D()(x) # å°† (Time, Features) å˜ä¸º (Features,)
    x = layers.Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=x, name="MiniResNet")
    return model

# å®ä¾‹åŒ–æ¨¡å‹
input_shape = (X_train.shape[1], X_train.shape[2]) # (260, 6)
model = build_mini_resnet(input_shape, num_classes)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# ==========================================
# 3. è®­ç»ƒ
# ==========================================
print("\nå¼€å§‹è®­ç»ƒ MiniResNet...")
history = model.fit(
    X_train, y_train,
    epochs=20,          # ResNeté€šå¸¸å¯ä»¥è®­ç»ƒæ›´å¤šè½®è€Œä¸é€€åŒ–ï¼Œå»ºè®®è®¾ä¸º 20-30
    batch_size=32,      # ç¨å¾®è°ƒå¤§ä¸€ç‚¹ batch size
    validation_data=(X_test, y_test)
)

# ==========================================
# 4. è¯„ä¼°ä¸è½¬æ¢
# ==========================================
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {acc*100:.2f}%")

if acc > 0.85: # è®¾å®šä¸€ä¸ªä¿å­˜é—¨æ§›
    print("âœ… æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¼€å§‹ä¿å­˜...")
    
    # 1. ä¿å­˜ Keras åŸç”Ÿæ¨¡å‹
    model.save('miniresnet_model.keras')
    
    # 2. è½¬æ¢ä¸º TFLite (é’ˆå¯¹ Mac ä¼˜åŒ–çš„ Concrete Function æ–¹å¼)
    print("æ­£åœ¨è½¬æ¢ä¸º TFLite...")
    try:
        # å®šä¹‰è¾“å…¥ç­¾åï¼Œå›ºå®š Batch Size = 1 (é€‚åˆå•æ¬¡æ¨ç†)
        run_model = tf.function(lambda x: model(x))
        concrete_func = run_model.get_concrete_function(
            tf.TensorSpec([1, input_shape[0], input_shape[1]], model.inputs[0].dtype)
        )

        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS, 
            tf.lite.OpsSet.SELECT_TF_OPS
        ]
        tflite_model = converter.convert()
        
        with open('miniresnet_model.tflite', 'wb') as f:
            f.write(tflite_model)
        print("ğŸ‰ TFLite æ¨¡å‹è½¬æ¢æˆåŠŸ: miniresnet_model.tflite")
        
    except Exception as e:
        print(f"âŒ æœ¬åœ°è½¬æ¢å¤±è´¥: {e}")
        print("è¯·ä½¿ç”¨ Google Colab å¹¶ä¸Šä¼  .keras æ–‡ä»¶è¿›è¡Œè½¬æ¢ã€‚")
else:
    print("âš ï¸ å‡†ç¡®ç‡æœªè¾¾åˆ°é¢„æœŸï¼Œä¸è¿›è¡Œä¿å­˜ã€‚")

# ç”»å›¾
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Test Acc')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Loss')
plt.legend()
plt.show()